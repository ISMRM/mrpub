<!doctype html><html lang=en><head><meta charset=utf-8><link rel=apple-touch-icon sizes=76x76 href=https://ismrm.github.io/mrpub/img/apple-icon.png><link rel=icon type=image/png href=https://ismrm.github.io/mrpub/img/favicon.png><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>MR-Pub</title><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,shrink-to-fit=no" name=viewport><link href="https://fonts.googleapis.com/css?family=Montserrat:400,700,200" rel=stylesheet><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css><link href=https://ismrm.github.io/mrpub/css/bootstrap.min.css rel=stylesheet><link href=https://ismrm.github.io/mrpub/sass/now-ui-kit.min.d685deb984313b2d3841640d3900ed08f52a6162d895259a65f0aed877187446.css rel=stylesheet><link href=https://ismrm.github.io/mrpub/css/demo.css rel=stylesheet><link rel=stylesheet href=https://ismrm.github.io/mrpub/css/highlight.min.css></head><body class="profile-page sidebar-collapse"><nav class="navbar navbar-expand-lg bg-primary fixed-top navbar-transparent" color-on-scroll=400><div class=container><div class=navbar-translate><a class=navbar-brand href=https://ismrm.github.io/mrpub/ rel=tooltip title data-placement=bottom>MR-Pub</a>
<button class="navbar-toggler navbar-toggler" type=button data-toggle=collapse data-target=#navigation aria-controls=navigation-index aria-expanded=false aria-label="Toggle navigation">
<span class="navbar-toggler-bar bar1"></span><span class="navbar-toggler-bar bar2"></span><span class="navbar-toggler-bar bar3"></span></button></div><div class="collapse navbar-collapse justify-content-end" id=navigation data-nav-image=/img/blurred-image-1.jpg><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://github.com/ISMRM/mrpub/issues/new><i class="now-ui-icons arrows-1_cloud-upload-94"></i><span>Submit</span></a></li></ul></div></div></nav><div class=wrapper><div class="page-header page-header-small" filter-color=nlgray><div class=page-header-image data-parallax=true style=background-image:url(https://ismrm.github.io/mrpub/img/header.jpg)></div></div><div class=section><div class=container><div class=button-container><a href=https://ismrm.github.io/mrpub/about style=background-color:#5fa8d7 class="btn btn-primary btn-round btn-lg" rel=tooltip title="Supported by ISMRM">About</a>
<a href="https://twitter.com/mrm_highlights?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" class="btn btn-default btn-round btn-lg btn-icon" rel=tooltip title="Follow us on Twitter"><i class="fa fa-twitter"></i></a></div><div class="container justify-content-center"><br><div class=row><div class=col-lg-12><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu15_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="DZNE Bonn, Germany">Leonie Henschel</a>
<a href=n/a data-toggle=tooltip target=_blank title="DZNE Bonn, Germany">Martin Reuter</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Deep learning</li><li>Segmentation</li></ul></li></ul></div><div class=description><a href=https://colab.research.google.com/github/Deep-MI/FastSurfer/blob/master/Tutorial/Tutorial_FastSurferCNN_QuickSeg.ipynb target=_blank><h2 style=color:#227ac2>FastSurfer - a fast and accurate deep-learning based neuroimaging pipeline</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_15 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_15><div class="card card-body" style=text-align:left>The rapid emergence of standardized robust non-invasive imaging methods and infrastructure for big data analysis has promoted the advent of a variety of large-scale neuroimaging studies. As a central component, versatile MR imaging offers the potential to investigate in-vivo the variability, development and anatomical layout of the human brain in health and disease. Increasing our knowledge of pre-symptomatic neuroanatomical changes supports research into disease etiology, risk and preserving factors, as well as potential intervention paradigms. Consequently, there is a need for efficient, scalable, and sensitive software tools to automatically extract clinically-relevant imaging markers. Traditional neuroimage analysis pipelines, however, often involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies.</div></div></p><p><a href=https://colab.research.google.com/github/Deep-MI/FastSurfer/blob/master/Tutorial/Tutorial_FastSurferCNN_QuickSeg.ipynb target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/deep-mi/FastSurfer target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu6_featured.png)></div><ul class=details><li class=author><a href=https://med.nyu.edu/faculty/jakob-asslaender data-toggle=tooltip target=_blank title="NYU School of Medicine">Jakob Assländer</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Non-cartesian</li><li>Reconstruction</li></ul></li></ul></div><div class=description><a href="https://mybinder.org/v2/gh/JakobAsslaender/2021_ISMRM_nonCartesianReconstruction_Exercises/HEAD?urlpath=pluto/open?path=/home/jovyan/src/nonCart_PlutoNotebook.jl" target=_blank><h2 style=color:#227ac2>Reconstruction of Non-Cartesian Data</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_6 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_6><div class="card card-body" style=text-align:left>Today, most clinical scans are performed with Cartesian k-space sampling due to its robustness and ease to implement acquisition and reconstruction. However, there are numerous reasons to use non-Cartesian sampling methods, reasons that range from robustness to motion and flow, to less intrusive undersampling artifacts and more beneficial properties for advanced image reconstruction methods such as compressed sensing. This lecture covers the basics of image reconstruction with the non-uniform FFT.</div></div></p><p><a href="https://mybinder.org/v2/gh/JakobAsslaender/2021_ISMRM_nonCartesianReconstruction_Exercises/HEAD?urlpath=pluto/open?path=/home/jovyan/src/nonCart_PlutoNotebook.jl" target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/JakobAsslaender/2021_ISMRM_nonCartesianReconstruction_Exercises target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu9_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="University Medical Center Göttingen">Moritz Bluementhal</a>
<a href=n/a data-toggle=tooltip target=_blank title="University Medical Center Göttingen">Nick Scholand</a>
<a href=http://wwwuser.gwdg.de/~muecker1 data-toggle=tooltip target=_blank title="University Medical Center Göttingen and DZK">Martin Uecker</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Reconstruction</li><li>Deep learning</li></ul></li></ul></div><div class=description><a href=https://colab.research.google.com/github/mrirecon/bart-workshop/blob/master/ismrm2021/neural_networks/bart_neural_networks.ipynb target=_blank><h2 style=color:#227ac2>Deep, Deep Learning with BART</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_9 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_9><div class="card card-body" style=text-align:left>Deep learning offers powerful tools for enhancing image quality and acquisition speed of MR images. Standard frameworks such as TensorFlow and PyTorch provide simple access to deep learning methods. However, they lack MRI specific operations and make reproducible research and code reuse more difficult due to fast changing APIs and complicated dependencies. In this tutorial, the user learns how to train a neural network with BART on the MNIST dataset. Moreover, the BART implementations of MoDL and the Variational Network are used for reconstruction.</div></div></p><p><a href=https://colab.research.google.com/github/mrirecon/bart-workshop/blob/master/ismrm2021/neural_networks/bart_neural_networks.ipynb target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/mrirecon/bart-workshop target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu13_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="Institute for Diagnostic and Interventional Radiology, University Medical Center Göttingen">Luo Guanxiong</a>
<a href=n/a data-toggle=tooltip target=_blank title="Institute for Diagnostic and Interventional Radiology, University Medical Center Göttingen">Nick Scholand</a>
<a href=n/a data-toggle=tooltip target=_blank title="Institute for Diagnostic and Interventional Radiology, University Medical Center Göttingen">Moritz Blumenthal</a>
<a href=n/a data-toggle=tooltip target=_blank title="Institute for Diagnostic and Interventional Radiology, University Medical Center Göttingen">Christian Holme</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Reconstruction</li><li>BART</li></ul></li></ul></div><div class=description><a href=https://colab.research.google.com/github/mrirecon/bart-workshop/blob/master/ismrm2021/bart_tensorflow/bart_tf.ipynb target=_blank><h2 style=color:#227ac2>Using data-driven image priors for image reconstruction with BART</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_13 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_13><div class="card card-body" style=text-align:left>Advanced reconstruction algorithms based on deep learning have recently drawn a lot of interest as they tend to outperform state-of-the-art methods. BART is a versatile framework for image reconstruction. In this work, we demonstrate how neural networks trained and tested with TensorFlow [5] can be integrated into BART. As an example, we discuss non-Cartesian parallel imaging using the SENSE model regularized by a log-likelihood image prior. The image prior is based on an autoregressive generative network pixel-cnn++. Furthermore, we validated the reconstruction pipeline using radial brain scans.</div></div></p><p><a href=https://colab.research.google.com/github/mrirecon/bart-workshop/blob/master/ismrm2021/bart_tensorflow/bart_tf.ipynb target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/mrirecon/bart-workshop/tree/master/ismrm2021 target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu14_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="Stanford University">Desai Arjun</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Deep learning</li><li>qMRI</li></ul></li></ul></div><div class=description><a href="https://colab.research.google.com/drive/1gU_qhdWnO46r0BUrQAtZ7FYlcogDDlAj?usp=sharing" target=_blank><h2 style=color:#227ac2>Image processing with DOSMA</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_14 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_14><div class="card card-body" style=text-align:left>This tutorial provides an introduction to DOSMA, a Python-based deep-learning (DL), open-source MR image analysis framework. Topics include: 1) streamlining vendor-agnostic, multi-format data I/O, 2) using and deploying DL tools, 3) parallelizing compute intensive routines (e.g. registration, curve fitting), and 4) building scalable, reproducible analysis workflows.</div></div></p><p><a href="https://colab.research.google.com/drive/1gU_qhdWnO46r0BUrQAtZ7FYlcogDDlAj?usp=sharing" target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/ad12/DOSMA target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu8_featured.png)></div><ul class=details><li class=author><a href=https://agahkarakuzu.github.io data-toggle=tooltip target=_blank title="NeuroPoly Research Lab, Polyrechnique Montreal. Montreal, Canada">Agah Karakuzu</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Python</li><li>Scientific computing</li></ul></li></ul></div><div class=description><a href=https://agahkarakuzu.github.io/sunrise/ target=_blank><h2 style=color:#227ac2>Scientific computing with Python</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_8 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_8><div class="card card-body" style=text-align:left>This interactive Jupyter Book is comprised of 3 notebooks, outlining an overarching standard operating procedure to work with 1D ( audio files of MRI pulse sequences, guitar melodies and vocal tracks), 2D (BIDS formatted reconstructed images) and 3D (multi-channel k-space data in ISMRM-RD format) data: i) obtain meta-information about the data and use community-developed readers wherever possible, ii) use NumPy to prepare the data for further processing and iii) use Scipy modules to perform fundamental signal and image processing tasks.</div></div></p><p><a href=https://agahkarakuzu.github.io/sunrise/ target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/agahkarakuzu/sunrise target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu5_featured.png)></div><ul class=details><li class=author><a href=www.mri.sbollmann.net data-toggle=tooltip target=_blank title="The University of Queensland">Bollmann Steffen</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>QSM</li></ul></li></ul></div><div class=description><a href="https://colab.research.google.com/drive/10iyYqhzF-U5NOTjIT_jqEr5mAH-aew-k?usp=sharing" target=_blank><h2 style=color:#227ac2>QSM: Theory & Methods</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_5 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_5><div class="card card-body" style=text-align:left>This educational lecture will give a broad overview of Quantitative Susceptibility Mapping. After explaining what magnetic susceptibility is and how we can measure it, I cover the data acquisition aspects, coil combination, unwrapping, masking, background field correction and dipole inversion.</div></div></p><p><a href="https://colab.research.google.com/drive/10iyYqhzF-U5NOTjIT_jqEr5mAH-aew-k?usp=sharing" target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/stebo85/Interactive-QSM-ISMRM-educational-talk target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu10_featured.png)></div><ul class=details><li class=author><a href=https://welcome.isr.tecnico.ulisboa.pt/author/andreiagaspar/ data-toggle=tooltip target=_blank title="Institute for Systems and Robotics - Lisboa and Department of Bioengineering">Andreia S. Gaspar</a>
<a href=https://welcome.isr.tecnico.ulisboa.pt/author/ritanunes/ data-toggle=tooltip target=_blank title="Institute for Systems and Robotics - Lisboa and Department of Bioengineering, Instituto Superior Técnico">Nuno A. Silva</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>T1 mapping</li><li>Pulse sequence</li></ul></li></ul></div><div class=description><a href=https://colab.research.google.com/github/ANG13/ProMyoT1/blob/main/PyProMyoT1_Python/pyProMyoT1.ipynb target=_blank><h2 style=color:#227ac2>ProMyoT1: Open-source Inversion recovery myocardial T1 mapping sequence for fast prototyping</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_10 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_10><div class="card card-body" style=text-align:left>Open-source Prototype of Myocardial T1 mapping (ProMyoT1) using pyPulseq which includes an inversion recovery T1 mapping sequence with a triggering scheme.</div></div></p><p><a href=https://colab.research.google.com/github/ANG13/ProMyoT1/blob/main/PyProMyoT1_Python/pyProMyoT1.ipynb target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/ANG13/ProMyoT1 target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu4_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="Rice University">Mohammad Zalbagi Darestani</a>
<a href=n/a data-toggle=tooltip target=_blank title="Rice University">Richard Heckel</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Reconstruction</li><li>CNN</li></ul></li></ul></div><div class=description><a href="https://colab.research.google.com/drive/1xu_NS6ClikkOM1TTPL7EDqOjQZCvCvlL#offline=true&sandboxMode=true" target=_blank><h2 style=color:#227ac2>Can Un-trained Networks Compete with Trained Ones for Accelerated MRI?</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_4 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_4><div class="card card-body" style=text-align:left>Convolutional Neural Networks (CNNs) are highly effective tools for image reconstruction problems. Typically, CNNs are trained on large amounts of images, but, perhaps surprisingly, even without any training data, CNNs such as the Deep Image Prior and Deep Decoder achieve excellent imaging performance. Here, we build on those works by proposing an un-trained CNN for accelerated MRI along with performance-enhancing steps including enforcing data-consistency and combining multiple reconstructions</div></div></p><p><a href="https://colab.research.google.com/drive/1xu_NS6ClikkOM1TTPL7EDqOjQZCvCvlL#offline=true&sandboxMode=true" target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/MLI-lab/ConvDecoder target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu12_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="Technical University of Munich Germany">Kerstin Hammernik</a>
<a href=n/a data-toggle=tooltip target=_blank title="University Hospital of Tübingen Germany">Thomas Küstner</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Reconstruction</li><li>Deep learning</li></ul></li></ul></div><div class=description><a href=https://ismrm-mit-cmr.github.io/CMR-DL-challenge target=_blank><h2 style=color:#227ac2>Deep learning reconstruction</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_12 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_12><div class="card card-body" style=text-align:left>The repository hosts some example codes to perform MR image reconstruction with deep learning architectures. We showcase a comparison of denoising networks (denoising) and unrolled reconstruction networks (physics-based). Pure real-valued processing (real) is compared against complex-valued processing as complex-valued operations (complex) or 2-channel real-valued operations (2chreal). Respective complex-valued operations and data consistency layers are provided. A deep plug-and-play prior example is illustrated for cardiovascular MRI.</div></div></p><p><a href=https://ismrm-mit-cmr.github.io/CMR-DL-challenge target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/ISMRM-MIT-CMR/CMR-DL-challenge target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu11_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="TU Delft, Delft, The Netherlands">Chiara Coletti</a>
<a href=n/a data-toggle=tooltip target=_blank title="TU Delft, Delft, The Netherlands">Maša Božić</a>
<a href=n/a data-toggle=tooltip target=_blank title="TU Delft, Delft, The Netherlands">Sebastian Weingärtner</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Pulse sequence</li><li>Simulator</li></ul></li></ul></div><div class=description><a href="https://colab.research.google.com/drive/1WpaUqp_CHLk4pG7kDvYlAcgjtGNpj_FG?usp=sharing" target=_blank><h2 style=color:#227ac2>Open-source MRI pulse sequence simulator</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_11 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_11><div class="card card-body" style=text-align:left>This notebook provides an intuitive, flexible and fully customizable MRI simulation tool for educational purposes. This version implements a spoiled GRE sequence applied to a numerical cardiac phantom with blood flow and simulates flow artefacts, as well as the effects of flow compensation.</div></div></p><p><a href="https://colab.research.google.com/drive/1WpaUqp_CHLk4pG7kDvYlAcgjtGNpj_FG?usp=sharing" target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/ISMRM-MIT-CMR/CMR-physics-challenge.git target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/edu7_featured.png)></div><ul class=details><li class=author><a href=n/a data-toggle=tooltip target=_blank title="Institute of Medical Engineering, Graz University of Technology">Oliver Maier</a>
<a href=n/a data-toggle=tooltip target=_blank title="Institute of Medical Engineering, Graz University of Technology">Stefan Spann</a>
<a href=n/a data-toggle=tooltip target=_blank title="Institute of Medical Engineering, Graz University of Technology">Markus Bödenler</a>
<a href=n/a data-toggle=tooltip target=_blank title="Institute of Medical Engineering, Graz University of Technology">Rudolf Stollberger</a></li><li class=date>2021-08-05</li><li class=tags><ul><li>Quantitative imaging</li><li>Parameter mapping</li></ul></li></ul></div><div class=description><a href="https://colab.research.google.com/drive/19BfSJmDPinZDY0m1sMAhETutIiJG3b33?usp=sharing" target=_blank><h2 style=color:#227ac2>PyQMRI: An accelerated Python based Quantitative MRI toolbox</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_7 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_7><div class="card card-body" style=text-align:left>To utilize full 3D information in advanced reconstruction and fitting algorithms on memory limited GPUs, special solutions strategies are necessary to leverage the speed advantage, e.g., hiding memory latency of repeated transfers to/from the GPU to host memory. This can be achieved using asynchronous execution strategies. However, correct synchronization of critical operations can be error prone. To this end, we propose PyQMRI, a simple to use Python toolbox for quantitative MRI.</div></div></p><p><a href="https://colab.research.google.com/drive/19BfSJmDPinZDY0m1sMAhETutIiJG3b33?usp=sharing" target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/IMTtugraz/PyQMRI target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/mp2rage_featured.png)></div><ul class=details><li class=author><a href=https://github.com/mathieuboudreau data-toggle=tooltip target=_blank title="Montreal Heart Institute, University of Montreal, Montreal, Canada">Mathieu Boudreau</a>
<a href=https://agahkarakuzu.github.io data-toggle=tooltip target=_blank title="NeuroPoly Lab, Ecole Polytechnique de Montreal, Montreal, Canada">Agah Karakuzu</a>
<a href=https://www.nist.gov/image-8494 data-toggle=tooltip target=_blank title="National Institute of Standards and Technology, MD, USA">Katy Keenan</a>
<a href=https://www.polymtl.ca/expertises/en/stikov-nikola data-toggle=tooltip target=_blank title="NeuroPoly Lab, Ecole Polytechnique de Montreal, Montreal, Canada">Nikola Stikov</a></li><li class=date>2021-03-26</li><li class=tags><ul><li>T1 mapping</li><li>Interactive tutorial</li></ul></li></ul></div><div class=description><a href=http://qmrlab.org/t1_book target=_blank><h2 style=color:#227ac2>Quantitative T1 mapping</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_1 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_1><div class="card card-body" style=text-align:left>This tutorial provides an introduction to quantitative T1 mapping, from an MRI physics perspective. Two widely used techniques are covered in-depth, Inversion Recovery and Variable Flip Angle (VFA), along with some discussions of cutting-edge variants of these techniques.</div></div></p><p><a href=http://qmrlab.org/t1_book target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/qMRLab/t1_book target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style="background-image:url(https://github.com/Notebook-Factory/PhaseUnwrapping_book/blob/master/_site/images/vol82_1.jpg?raw=true)"></div><ul class=details><li class=author><span data-toggle=tooltip title="Department of Radiological Sciences University of California Los Angeles California">Michael Loecher</span>
<span data-toggle=tooltip title="Department of Bioengineering University of California Los Angeles California">Patrick Magrath</span>
<span data-toggle=tooltip title="Department of Biomedical Physics University of California Los Angeles California">Eric Aliotta</span>
<span data-toggle=tooltip title="Department of Radiological Sciences University of California Los Angeles California">Daniel B. Ennis</span></li><li class=date>2020-03-26</li><li class=tags><ul><li>Phase unwrapping</li><li>Low SNR</li></ul></li></ul></div><div class=description><a href=https://notebook-factory.github.io/cvxflow_book1/intro.html target=_blank><h2 style=color:#1e3059>Time‐optimized 4D phase contrast MRI with real‐time convex optimization of gradient waveforms and fast excitation methods</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#101002mrm27716 aria-expanded=false aria-controls=collapseExample>Abstract</a><div class=collapse id=101002mrm27716><div class="card card-body" style=text-align:left>To shorten 4D flow acquisitions by shortening TRs with fast RF pulses and gradient waveforms. Real‐time convex optimization is used to generate these gradients waveforms on the scanner.</div></div></p><p><a href=https://notebook-factory.github.io/cvxflow_book1/intro.html target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/mloecher/cvxflow target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a>
<a href=https://doi.org/10.1002/mrm.27716 target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/MRM-10.1002/mrm.27716-1e3059.svg?logo=PubMed&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(https://github.com/mriphysics/ihMT_steadystate/raw/master/figs/mb_fig.jpg)></div><ul class=details><li class=author><a href=http://orcid.org/0000-0001-8925-9032 data-toggle=tooltip target=_blank title="School of Biomedical Engineering and Imaging Sciences King's College London London United Kingdom">Shaihan J. Malik</a>
<a href=http://orcid.org/0000-0001-6508-9315 data-toggle=tooltip target=_blank title="School of Biomedical Engineering and Imaging Sciences King's College London London United Kingdom">Rui P. A. G. Teixeira</a>
<a href=http://orcid.org/0000-0002-0246-5851 data-toggle=tooltip target=_blank title="School of Biomedical Engineering and Imaging Sciences King's College London London United Kingdom">Daniel J. West</a>
<a href=http://orcid.org/0000-0001-7640-5520 data-toggle=tooltip target=_blank title="Neuroimaging Department Institute of Psychiatry, Psychology &amp;amp; Neuroscience King's College London London United Kingdom">Tobias C. Wood</a>
<a href=http://orcid.org/0000-0002-2690-5495 data-toggle=tooltip target=_blank title="School of Biomedical Engineering and Imaging Sciences King's College London London United Kingdom">Joseph V. Hajnal</a></li><li class=date>2020-03-26</li><li class=tags><ul><li>ihMT</li><li>Multiband RF</li></ul></li></ul></div><div class=description><a href=https://nbviewer.jupyter.org/github/Notebook-Factory/ihMT_notebooks/blob/main/domasno_notebook.ipynb target=_blank><h2 style=color:#1e3059>Steady‐state imaging with inhomogeneous magnetization transfer contrast using multiband radiofrequency pulses</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#101002mrm27984 aria-expanded=false aria-controls=collapseExample>Abstract</a><div class=collapse id=101002mrm27984><div class="card card-body" style=text-align:left>Inhomogeneous magnetization transfer (ihMT) is an emerging form of MRI contrast that may offer high specificity for myelinated tissue. Existing ihMT and pulsed MT sequences often use separate radiofrequency pulses for saturation and signal excitation. This study investigates the use of nonselective multiband radiofrequency pulses for simultaneous off‐resonance saturation and on‐resonance excitation specifically for generation of ihMT contrast within rapid steady‐state pulse sequences.</div></div></p><p><a href=https://nbviewer.jupyter.org/github/Notebook-Factory/ihMT_notebooks/blob/main/domasno_notebook.ipynb target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/mriphysics/ihMT_steadystate target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a>
<a href=https://doi.org/10.1002/mrm.27984 target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/MRM-10.1002/mrm.27984-1e3059.svg?logo=PubMed&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(https://onlinelibrary.wiley.com/cms/asset/71190c80-5a97-47aa-a4fe-bd5f29621b39/mrm28384-fig-0007-m.png)></div><ul class=details><li class=author><a href=http://orcid.org/0000-0002-6533-1113 data-toggle=tooltip target=_blank title="Department of Radiology Stanford University Stanford CA USA">Michael Loecher</a>
<span data-toggle=tooltip title="Department of Radiology Stanford University Stanford CA USA">Matthew J. Middione</span>
<span data-toggle=tooltip title="Department of Radiology Stanford University Stanford CA USA">Daniel B. Ennis</span></li><li class=date>2020-03-26</li><li class=tags><ul><li>Gradient</li><li>Optimization</li></ul></li></ul></div><div class=description><a href=https://notebook-factory.github.io/gropt_book/intro target=_blank><h2 style=color:#1e3059>A gradient optimization toolbox for general purpose time‐optimal MRI gradient waveform design</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#101002mrm28384 aria-expanded=false aria-controls=collapseExample>Abstract</a><div class=collapse id=101002mrm28384><div class="card card-body" style=text-align:left>To introduce and demonstrate a software library for time‐optimal gradient waveform optimization with a wide range of applications. The software enables direct on‐the‐fly gradient waveform design on the scanner hardware for multiple vendors.</div></div></p><p><a href=https://notebook-factory.github.io/gropt_book/intro target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/cmr-group/gropt target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a>
<a href=https://doi.org/10.1002/mrm.28384 target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/MRM-10.1002/mrm.28384-1e3059.svg?logo=PubMed&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(https://gitlab.com/notZaki/rrift/-/raw/master/imgs/gbmCurves.png)></div><ul class=details><li class=author><a href=http://orcid.org/0000-0001-5648-0590 data-toggle=tooltip target=_blank title="Medical Physics Unit McGill University Montreal Canada">Zaki Ahmed</a>
<a href=http://orcid.org/0000-0002-0546-1733 data-toggle=tooltip target=_blank title="Medical Physics Unit McGill University Montreal Canada">Ives R. Levesque</a></li><li class=date>2020-03-26</li><li class=tags><ul><li>DCE</li><li>Cancer</li><li>Perfusion</li></ul></li></ul></div><div class=description><a href=https://notebook-factory.github.io/RRIFT_page/RRIFT.html target=_blank><h2 style=color:#1e3059>Pharmacokinetic modeling of dynamic contrast‐enhanced MRI using a reference region and input function tail</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#101002mrm27913 aria-expanded=false aria-controls=collapseExample>Abstract</a><div class=collapse id=101002mrm27913><div class="card card-body" style=text-align:left>Quantitative analysis of dynamic contrast‐enhanced MRI (DCE‐MRI) requires an arterial input function (AIF) which is difficult to measure. We propose the reference region and input function tail (RRIFT) approach which uses a reference tissue and the washout portion of the AIF.</div></div></p><p><a href=https://notebook-factory.github.io/RRIFT_page/RRIFT.html target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/Notebook-Factory/RRIFT_page target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a>
<a href=https://doi.org/10.1002/mrm.27913 target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/MRM-10.1002/mrm.27913-1e3059.svg?logo=PubMed&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(https://notebook-factory.github.io/mri-course/_images/main-2_18_0.png)></div><ul class=details><li class=author><a href=https://bme.engineering.arizona.edu/faculty-staff/faculty/nan-kuei-chen data-toggle=tooltip target=_blank title="The University of Arizona College of Engineering, Tucson, AZ, USA">Nan-kuei Chen</a></li><li class=date>2020-03-26</li><li class=tags><ul><li>Reconstruction</li><li>DTI basics</li><li>RF pulse</li><li>Fitting</li></ul></li></ul></div><div class=description><a href=https://notebook-factory.github.io/mri-course/intro.html target=_blank><h2 style=color:#227ac2>MRI Online Course</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_3 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_3><div class="card card-body" style=text-align:left>This online course is divided in 4 chapters (for more info see 📕 Course chapters ) that demonstrate different MRI techniques and display the results in Jupyer notebooks. The notebooks are written in Julia (1.4.1) and Python (3.7).</div></div></p><p><a href=https://notebook-factory.github.io/mri-course/intro.html target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/Notebook-Factory/mri-course target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style=background-image:url(img/edu/sct_featured.png)></div><ul class=details><li class=author><a href=https://github.com/mathieuboudreau data-toggle=tooltip target=_blank title="Montreal Heart Institute, University of Montreal, Montreal, Canada">Mathieu Boudreau</a>
<a href=https://github.com/jcohenadad data-toggle=tooltip target=_blank title="NeuroPoly Lab, Ecole Polytechnique de Montreal, Montreal, Canada">Julien Cohen-Adad</a></li><li class=date>2020-03-26</li><li class=tags><ul><li>Spinal cord</li><li>Analysis pipeline</li></ul></li></ul></div><div class=description><a href=https://mathieuboudreau.github.io/pipelines-jupyter-book/01/sct_mtsat target=_blank><h2 style=color:#227ac2>Image processing with Spinal Cord Toolbox (SCT)</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#edu_2 aria-expanded=false aria-controls=collapseExample>Educational Content</a><div class=collapse id=edu_2><div class="card card-body" style=text-align:left>This notebook presents an example analysis pipeline using the Spinal Cord Toolbox (SCT), a suite of tools specialized for analysis of spinal cord MRI images of the spinal. Topics covered include: segmentation, masking, registration, warping, and quantitative metric computation. This tutorial was generated in a Jupyter Notebook and coded in Python.</div></div></p><p><a href=https://mathieuboudreau.github.io/pipelines-jupyter-book/01/sct_mtsat target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/mathieuboudreau/pipelines-jupyter-book target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a></p></div></div><hr><div class="blog-card alt"><div class=meta><div class=photo style="background-image:url(https://github.com/Notebook-Factory/PhaseUnwrapping_book/blob/master/_site/images/vol82_1.jpg?raw=true)"></div><ul class=details><li class=author><a href=http://orcid.org/0000-0001-6564-4219 data-toggle=tooltip target=_blank title="Department of Biomedical Engineering University of Arizona Tucson Arizona">Nan‐kuei Chen</a>
<span data-toggle=tooltip title="Graduate Institute of Biomedical Electronics and Bioinformatics National Taiwan University Taipei Taiwan">Pei‐Hsin Wu</span></li><li class=date>2020-03-26</li><li class=tags><ul><li>Phase unwrapping</li><li>Low SNR</li></ul></li></ul></div><div class=description><a href=https://zelenkastiot.github.io/Phase_book/intro.html target=_blank><h2 style=color:#1e3059>The use of Fourier‐domain analyses for unwrapping phase images of low SNR</h2></a><p></p><p><a class="btn btn-primary" style="padding:2px 20px 1px;color:#fff;height:25px;text-align:center;background-color:#5fa8d7" data-toggle=collapse href=#101002mrm27719 aria-expanded=false aria-controls=collapseExample>Abstract</a><div class=collapse id=101002mrm27719><div class="card card-body" style=text-align:left>We report a new postprocessing procedure that uses Fourier‐domain data analyses to improve the accuracy and reliability of phase unwrapping for MRI data of low SNR.</div></div></p><p><a href=https://zelenkastiot.github.io/Phase_book/intro.html target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/Interactive-Jupyter%20Book-1e3059.svg?logo=Jupyter&logoWidth=25&style=social"></a>
<a href=https://github.com/mloecher/cvxflow target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/GitHub-Code-1f2428.svg?logo=github&logoColor=&logoWidth=25&style="></a>
<a href=https://doi.org/10.1002/mrm.27719 target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:25px src="https://img.shields.io/badge/MRM-10.1002/mrm.27719-1e3059.svg?logo=PubMed&logoColor=&logoWidth=25&style="></a></p></div></div></div></div></div><br></div></div><footer class=footer data-background-color=black><div class=container><nav><ul><li><a href=https://twitter.com/mrm_highlights><i class="fa fa-twitter"></i></a></li><li><a href=https://github.com/ismrm><i class="fa fa-github"></i></a></li><li><a href=https://spdx.org/licenses/MIT><i class="fa fa-balance-scale"></i>MIT</a></li></ul></nav></div></footer></div></body><script src=https://ismrm.github.io/mrpub/js/core/jquery.3.2.1.min.js type=text/javascript></script><script src=https://ismrm.github.io/mrpub/js/core/popper.min.js type=text/javascript></script><script src=https://ismrm.github.io/mrpub/js/core/bootstrap.min.js type=text/javascript></script><script src=https://ismrm.github.io/mrpub/js/plugins/bootstrap-switch.js></script><script src=https://ismrm.github.io/mrpub/js/plugins/nouislider.min.js type=text/javascript></script><script src=https://ismrm.github.io/mrpub/js/plugins/bootstrap-datepicker.js type=text/javascript></script><script src="https://ismrm.github.io/mrpub/js/now-ui-kit.js?v=1.1.0" type=text/javascript></script><script type=text/javascript>$(document).ready(function(){nowuiKit.initSliders()});function scrollToDownload(){$('.section-download').length!=0&&$("html, body").animate({scrollTop:$('.section-download').offset().top},1e3)}</script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/x-mathjax-config>
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script><script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script></html>